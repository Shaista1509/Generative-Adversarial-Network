{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84bfe4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1 — Setup & Folders\n",
    "import os, time, random, math, json, glob\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, utils\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# Reproducibility\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Output layout\n",
    "OUT_DIR = Path(\"./gan_outputs\")\n",
    "(OUT_DIR / \"images\").mkdir(parents=True, exist_ok=True)\n",
    "(OUT_DIR / \"checkpoints\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e777ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2 — CIFAR-10 Data\n",
    "DATA_ROOT   = \"./data\"\n",
    "img_size    = 32     # CIFAR-10 is 32x32\n",
    "img_channels= 3      # RGB\n",
    "batch_size  = 128\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(img_size),\n",
    "    transforms.CenterCrop(img_size),\n",
    "    transforms.ToTensor(),                               # [0,1]\n",
    "    transforms.Normalize([0.5, 0.5, 0.5],                # -> [-1,1]\n",
    "                         [0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "trainset = datasets.CIFAR10(root=DATA_ROOT, train=True, download=True, transform=transform)\n",
    "loader   = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
    "\n",
    "print(f\"Train images: {len(trainset)} | Batches/epoch: {len(loader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32069610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3 — Hyper-parameters\n",
    "z_dim   = 128\n",
    "g_width = 64\n",
    "d_width = 64\n",
    "lr      = 2e-4\n",
    "beta1   = 0.5\n",
    "beta2   = 0.999\n",
    "epochs  = 30\n",
    "\n",
    "# Fixed noise to visualize progress\n",
    "fixed_z = torch.randn(64, z_dim, 1, 1, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3044618",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4 — Generator\n",
    "class Generator(nn.Module):\n",
    "    \"\"\"\n",
    "    Input:  (N, z_dim, 1, 1)\n",
    "    Output: (N, 3, 32, 32) in [-1, 1]\n",
    "    1x1 -> 4x4 -> 8x8 -> 16x16 -> 32x32\n",
    "    \"\"\"\n",
    "    def __init__(self, z_dim=128, img_channels=3, g_width=64):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            # 1x1 -> 4x4\n",
    "            nn.ConvTranspose2d(z_dim, g_width*4, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(g_width*4),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            # 4x4 -> 8x8\n",
    "            nn.ConvTranspose2d(g_width*4, g_width*2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(g_width*2),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            # 8x8 -> 16x16\n",
    "            nn.ConvTranspose2d(g_width*2, g_width, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(g_width),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            # 16x16 -> 32x32\n",
    "            nn.ConvTranspose2d(g_width, img_channels, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.net(z)\n",
    "\n",
    "G = Generator(z_dim=z_dim, img_channels=img_channels, g_width=g_width).to(device)\n",
    "G\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6d86f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5 — Discriminator\n",
    "class Discriminator(nn.Module):\n",
    "    \"\"\"\n",
    "    Input:  (N, 3, 32, 32)\n",
    "    Output: (N, 1) logits (use BCEWithLogitsLoss)\n",
    "    32 -> 16 -> 8 -> 4 -> 1\n",
    "    \"\"\"\n",
    "    def __init__(self, img_channels=3, d_width=64):\n",
    "        super().__init__()\n",
    "        # First block: no BatchNorm (DCGAN convention)\n",
    "        self.conv_in = nn.Sequential(\n",
    "            nn.Conv2d(img_channels, d_width, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "        self.conv_mid = nn.Sequential(\n",
    "            nn.Conv2d(d_width, d_width*2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(d_width*2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(d_width*2, d_width*4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(d_width*4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "        )\n",
    "        self.conv_out = nn.Conv2d(d_width*4, 1, 4, 1, 0, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_in(x)\n",
    "        x = self.conv_mid(x)\n",
    "        x = self.conv_out(x)     # (N,1,1,1)\n",
    "        return x.view(-1, 1)     # (N,1)\n",
    "\n",
    "D = Discriminator(img_channels=img_channels, d_width=d_width).to(device)\n",
    "D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebf730b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6 — Losses & Optimizers\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "opt_G = torch.optim.Adam(G.parameters(), lr=lr, betas=(beta1, beta2))\n",
    "opt_D = torch.optim.Adam(D.parameters(), lr=lr, betas=(beta1, beta2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44aa671a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7 — Training Loop\n",
    "def train_gan(G, D, loader, epochs, fixed_z, log_interval=100):\n",
    "    G.train(); D.train()\n",
    "    for epoch in range(1, epochs+1):\n",
    "        g_sum, d_sum = 0.0, 0.0\n",
    "        t0 = time.time()\n",
    "\n",
    "        for i, (real, _) in enumerate(loader):\n",
    "            real = real.to(device)\n",
    "            N = real.size(0)\n",
    "            real_labels = torch.ones(N, 1, device=device)\n",
    "            fake_labels = torch.zeros(N, 1, device=device)\n",
    "\n",
    "            # -------- Train D --------\n",
    "            # Real\n",
    "            d_real = D(real)\n",
    "            d_loss_real = criterion(d_real, real_labels)\n",
    "\n",
    "            # Fake\n",
    "            z = torch.randn(N, z_dim, 1, 1, device=device)\n",
    "            fake = G(z)\n",
    "            d_fake = D(fake.detach()) \n",
    "            d_loss_fake = criterion(d_fake, fake_labels)\n",
    "\n",
    "            d_loss = d_loss_real + d_loss_fake\n",
    "            opt_D.zero_grad()\n",
    "            d_loss.backward()\n",
    "            opt_D.step()\n",
    "\n",
    "            # -------- Train G --------\n",
    "            d_fake_for_g = D(fake)  # no detach\n",
    "            g_loss = criterion(d_fake_for_g, real_labels)\n",
    "\n",
    "            opt_G.zero_grad()\n",
    "            g_loss.backward()\n",
    "            opt_G.step()\n",
    "\n",
    "            g_sum += g_loss.item()\n",
    "            d_sum += d_loss.item()\n",
    "\n",
    "            if (i + 1) % log_interval == 0:\n",
    "                print(f\"Epoch {epoch:02d} [{i+1:04d}/{len(loader)}] \"\n",
    "                      f\"D_loss={d_loss.item():.3f} G_loss={g_loss.item():.3f}\")\n",
    "\n",
    "        # Save sample grid after each epoch\n",
    "        G.eval()\n",
    "        with torch.no_grad():\n",
    "            fake_fixed = G(fixed_z).cpu()\n",
    "        G.train()\n",
    "\n",
    "        grid_path = OUT_DIR / \"images\" / f\"epoch_{epoch:03d}.png\"\n",
    "        utils.save_image(fake_fixed, grid_path, nrow=8, normalize=True, value_range=(-1, 1))\n",
    "\n",
    "        print(f\"Epoch {epoch:02d} | time={time.time()-t0:.1f}s | \"\n",
    "              f\"mean D={d_sum/len(loader):.3f} | mean G={g_sum/len(loader):.3f} | \"\n",
    "              f\"saved {grid_path}\")\n",
    "\n",
    "train_gan(G, D, loader, epochs=epochs, fixed_z=fixed_z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1455dd72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8 — View the latest saved grids\n",
    "def show_latest(n=4):\n",
    "    paths = sorted(glob.glob(str(OUT_DIR / \"images\" / \"epoch_*.png\")))\n",
    "    for p in paths[-n:]:\n",
    "        img = Image.open(p)\n",
    "        plt.figure(figsize=(4,4))\n",
    "        plt.imshow(img)\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(os.path.basename(p))\n",
    "\n",
    "show_latest(4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94ccfd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9 — Save/Load checkpoints (optional)\n",
    "def save_ckpt(epoch):\n",
    "    torch.save({\n",
    "        \"G\": G.state_dict(),\n",
    "        \"D\": D.state_dict(),\n",
    "        \"epoch\": epoch,\n",
    "        \"hparams\": dict(z_dim=z_dim, g_width=g_width, d_width=d_width,\n",
    "                        lr=lr, betas=(beta1,beta2), img_size=img_size, img_channels=img_channels)\n",
    "    }, OUT_DIR / \"checkpoints\" / f\"gan_epoch_{epoch:03d}.pt\")\n",
    "\n",
    "def load_ckpt(path):\n",
    "    ckpt = torch.load(path, map_location=device)\n",
    "    G.load_state_dict(ckpt[\"G\"])\n",
    "    D.load_state_dict(ckpt[\"D\"])\n",
    "    print(\"Loaded epoch:\", ckpt[\"epoch\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecbe7418",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10 — Simple experiment logger\n",
    "LOG_PATH = OUT_DIR / \"experiment_log.jsonl\"\n",
    "\n",
    "def log_experiment(name, params, notes):\n",
    "    rec = dict(\n",
    "        name=name,\n",
    "        params=params,\n",
    "        notes=notes,\n",
    "        timestamp=time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        images_dir=str(OUT_DIR / \"images\"),\n",
    "        ckpt_dir=str(OUT_DIR / \"checkpoints\"),\n",
    "    )\n",
    "    with open(LOG_PATH, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(json.dumps(rec) + \"\\n\")\n",
    "    print(\"Logged:\", name)\n",
    "\n",
    "# Example: log the baseline run (edit notes after you view images)\n",
    "log_experiment(\n",
    "    \"Run-A (Baseline CIFAR10, DCGAN)\",\n",
    "    dict(dataset=\"CIFAR10\", img_size=img_size, img_channels=img_channels,\n",
    "         z_dim=z_dim, g_width=g_width, d_width=d_width, lr=lr, betas=(beta1,beta2),\n",
    "         batch_size=batch_size, epochs=epochs, loss=\"BCEWithLogits\"),\n",
    "    notes=\"Baseline. Check grids at epoch_005/010/020/030 to discuss convergence.\"\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
